---
layout:     post
title:      "第四周python机器学习预测分析学习笔记"
date:       2019-03-28
author:     "桂志强"
header-img: "img/post-bg-2015.jpg"
tags:
    - python
    - 学习笔记
---



###       第四周python机器学习预测分析学习笔记



### 第一章 关于预测的两类核心算法

#### 1.0概括

涉及问题主要是“**函数逼近**”问题，是**有监督学习**的一个子集，常见解决算法为**线性回归**和**逻辑回归**。它包含了各种分类问题和回归问题。解决函数逼近问题的两类算法：**惩罚性回归和集成方法**。

#### 1.1 两类算法的简介

**惩罚性回归方法**：是由普通最小二乘法（ordinary least squares，OLS）衍生出来的，克服了最小二乘法的**过拟合问题**。惩罚线性回归可以减少自由度使之与数据规模、问题的复杂度相匹配。

**集成方法**：基本思想是构 建多个不同的**预测模型**，然后将其输出做某种组合作为最终的输出，，如取平均值或采用多 数人的意见（投票）。单个预测模型叫作**基学习器**。集成方法通常将**二元决策树**作为它的基学习器。

​    --投票：一种获得不同模型的方法，先对训练数据随机取样，然后基于这些随机数据子集进行训练。此方法可以产生大量的具有稍许差异 的二元决策树。

#### 1.2 两种算法的优劣

惩罚性回归方法训练速度，预测速度快，可以处理大量特征，但对于问题复杂度劣于集成方法。集成方法训练速度，预测速度，处理大量特征都逊于惩罚性回归方法，但对于问题复杂度优于惩罚性回归方法。

#### 1.3 构建预测模型的流程

+ 提取或组合预测所需的特征。 

+ 设定训练目标。 

+ 训练模型。 

+ 评估模型在测试数据上的性能表现

  <!-- 在完成第一遍过程后，可以通过选择不同的特征集、不同的目标等手段来提高预测的性能-->

具体来说：

#####       1.3.1 **构建一个机器学习问题**

+ 需要将实际问题转换成可测量可优化的具体目标，

+ 接下来需要搜集数据

+ 定量的训练目标，或者部分任务是数据提取

  <!-- 通常构造一个机器学习问题可以采用不同的方法。这就导致了问题的构造、模型的选择、模型的训练、模型性能评估这一过程会发生多次迭代-->

#####       1.3.2 **特征提取和特征工程**

+ **特征提取**就是一个把自由形式的各种数据转换成行、列形式的数字的过程。

+ **特征工程**就是对特征进行整理组合，以达到更富有信息量的过程。

#####     1.3.3 **训练模型**

+ 开始都是先选择作为基线的特征集合,通常训练 100 ～ 5000 个不同的模型
+ 然后从中精选出一个模型进行部署

   1.3.4 **确定训练后模型的性能 **

+ 留出一部分数据，不用于训练，用于模型的测试
+ 用这部分数据集测试算法的性能。

#### 小结

介绍了本书要解决的问题以及构建预测模型的处理流程。本书关注两类算法族。 

介绍了构建一个预测模型的步骤，每个步骤的各种选择的权衡，对输出结果的考虑。非模型训练时使用的数据可以用来评估预测模型。 



### 第二章 通过理解数据来了解问题

#### 2.0 概括

熟悉**数据集**，展示Python 中分析数据的工具包。 回顾基础问题的架构、术语、机器学习数据集的特性

有几类不同的**函数逼近问题**。

#### 2.1 “解剖”一个新问题

数据是按照行和列组织的。每行代表一个**实例**，每一列代表一个**属性**，**标签**代表需要预测的数据。

机器学习算法使用**属性**来预测**标签**。 属性用于预测。标签是观察得到的结果，机器学习基于此来构建预测模型。

机器学习的技巧构建的模型需有**泛化能力**（即可以解决新的实例），

**训练**：是一个构建预测模型的过程，依赖于算法。**基本思想**：假定属性和标签之间存在可预测的关系，观察出错的情况，做出修正，然后重复此过程直到获得一个相对满意的模型

##### 2.1.1 属性和标签的不同类型决定模型的选择 

**属性**：分成两类。一是**数值变量**，二是**类别（因素）变量**。

+ 数值变量：<u>（惩罚回归算法只能处理数值变量）</u>
+ 类别变量：具有不同值之间**没有顺序关系**的特点，

**数值标签与类别标签**：

+ 数值标签：属于**回归**问题
+ 类别标签：属于**分类**问题。（如果分类结果只取 2个值，叫作二元分类问题。取多个值，就是多类别分类问题。 ）

##### 2.1.2 新数据集的注意事项 

需要检查的事项:

+  行数、列数(数据规模)
+ 类别变量的数目、类别的取值范围 
+ 缺失的值
+ 属性和标签的统计特性 



**遗失值插补**（imputation）：最简单方法就是用每行所有此项的值的平均值来代替遗失的值。

#### 2.2　分类问题（例子）：“用声纳发现未爆炸的水雷”

1. 搜集数据，代码如下：

```
   import urllib2
   import sys

	target_url = ("https://archive.ics.uci.edu/ml/machine-learning-" 	"databases/undocumented/connectionist-bench/sonar/sonar.all-data")

	data = urllib2.urlopen(target_url)

```

​	可以查看data的行列数，判断数据规模，进而可以大致判断训练所需时间，所需方法。值得注意的是，通	常，**惩罚线性回归**的训练时间远小于**集成方法**。

​	而如果数据集的列数远远大于行数，那么采用**惩罚线性回归**的方法则有很大的可能获得最佳的预测，反之亦	然。

##### 2.2.2　“岩石 vs. 水雷”数据集统计特征

2. 确定数值类型，判断数值型，类别型（通常，类别型变量用字符串表示）。而在某些情况下，二值类别变量（例如true和false）可以表示成０和 1。

3. 再下一步就是获得**数值型**属性的**描述性统计信息**和类别型属性具体类别的**数量分布**。

   + 计算属性的均值和方差等（作用是加强建立预测模型时的直观感受）

   + 找出异常值：

     代码如下：

     ```python
     	ntiles = 10
     	percentBdry = []
     	for i in range(ntiles+1):    
             percentBdry.append(np.percentile(colArray, i*(100)/ntiles))
             #np.percentile,计算一个多维数组的任意百分比分位数，第二个参数是位数。且百分位是从小		 #到大顺序排列。
     	sys.stdout.write("Boundaries for 10 Equal Percentiles \n") 					     print(percentBdry) 
         sys.stdout.write(" \n")
     ```

     具体方法是：：将一组数字按照百分位数进行划分（有序），例如，第 25 百 分位数是含有最小的25% 的数，这样很容易找到边界。将 数组按照1/4、1/5、1/10 划分的百分位数**四分位数**，**五分位数**，**十分位数**。 然后找到跨度，找出异常。（注意，有时有些异常是正常的）

   ###### 用分位数图展示异常点 

   ​	更具体的研究异常点：数据分布图。

   ​	<!--分布图展示了数据的**百分位边界**与**高斯分布**的同样百分位的边界对比。-->

##### 2.2.4　类别属性的统计特征 

​	4. 通过**分层抽样**，**二元决策树算法**（如随机森林算法）等方法。

##### 2.2.5　利用 Python Pandas 对“岩石 vs. 水雷”数据集进行统计分析 

 5. **pandas.DataFrame**数据结构：行代表实例，列代表属性。

    Pandas 可以自动计算出均值、方差、分位数(具体操作省略)

#### 2.3 可视化展示（分类问题）

可视化可以提供对数据的直观感受，但值得注意的是，分类问题和回归问题的可视化有所不同。

##### 2.3.1　利用平行坐标图进行可视化展示 

​	平行坐标图（parallel coordinates plot）：具有多个属性问题的一种可视化方法

​	数据集的 平行坐标图对于数据集中的每一行属性都有对应的一条折线。基于标签对折线 标示不同的颜色，     	更有利于观测到属性 值与标签之间的关系。

​	有关代码如下：	

```
		for i in range(208):   
        #assign color based on "M" or "R" labels   
        	if rocksVMines.iat[i,60] == "M":     
       		 pcolor = "red"    else:      
       		 pcolor = "blue"
   			 #plot rows of data as if they were series data   
   			 dataRow = rocksVMines.iloc[i,0:60]   
    		 dataRow.plot(color=pcolor)
    	
    	plot.xlabel("Attribute Index")
        plot.ylabel(("Attribute Values")) 
        plot.show()

```

##### 2.3.2　属性和标签的关系可视化

属性之间的关系：属性与标签的**交会图（又叫作散点图，scatter plots）**，展示了属性对之间关系的密切程度。

<!--代码省略。-->

到一个交会图常见的问题：当其中一个变量只取有限的几个值时，很多点会重叠在一起。解决方法：每个点都加上一个小的随机数，产生**少量的离散值**。（分类器）

两个属性（或一个属性、一个标签）的相关程度可以由**皮尔逊相关系数**（Pearson’ s correlation coefﬁcient）来量化：

+ 给定 2 个等长的向量 u 和 v
+ 首先 u 的所有元素都减去 u 的均值
+ 对 v 也做同样的事情
+ 以向量 ∆u 相同的定义方式，对应第二个向量 v，定义向量 ∆v。 

+ 则 u 和 v 之间的皮尔森相关系数为：
+ <!--待加-->

<!--具体使用代码省略-->



##### **2.3.3　用热图（heat map）展示属性和标签的相关性** 

如果有大量属性，便无法使用**散点图**！

这时需要获得大量属性之间相关性的一种方法便是：

+ 计算出每对属性的皮尔森相关系数

+ 将相关系数构成一个矩阵，矩阵的第 ij-th 个元素对应第 i 个属性与第 j 个属性的相关系数，

+ 然后把这些矩阵元素画到**热图**上。

代码实现如下：

```
	rocksVMines = pd.read_csv(target_url,header=None, prefix="V")
	#calculate correlations between real-valued attributes 
	corMat =  DataFrame(rocksVMines.corr())
	#visualize correlations using heatmap
	plot.pcolor(corMat) 
	plot.show()
```



#### 回归问题：基于因素变量的实数值预测

除基本的统计信息之外，还有：

##### 盒型图：

**箱线图(盒型图)**是一种比打印出数据更快、更直接的发现异常点的方法！

功能：

 	1. 直观明了地识别数据批中的异常值
 	2. 利用盒形图判断数据批的偏态和尾重
 	3. 利用盒形图比较几批数据的形状

不足：

 	1. 不能提供关于数据分布偏态和尾重程度的精确度量。
 	2. 对于批量比较大的数据批，反应的形状信息更加模糊。
 	3. 用中位数代表总体评价水平有一定的局限性。

<!--来自百度百科-->



##### 属性值归一化（normalization）：

归一化指确定每列数据的**中心**，然后对数值进行缩放，使属性1 的一个单位值与属 性 2 的一个单位值相同。

代码示例如下：

```
		for i in range(8):   
        	mean = summary.iloc[1, i]   
       	    sd = summary.iloc[2, i]
       	    abaloneNormalized.iloc[:,i:(i + 1)] = (abaloneNormalized.iloc[:,i:(i + 1)] - 		     mean) / sd                    	               
		array3 = abaloneNormalized.values 
```



##### 回归问题的平行坐标图：变量关系可视化 

比较：

​	**分类问题**：平行坐标图对于此类问题，折线代表了一行数据，折线的颜色表明了其所属的类别。这有利于可	视化属性和所属类别之间的关系

​	**回归问题**：应用不同的颜色来对应标签值的高低。实现由标签的实数值到颜色值的映射，将标签的实数值压	缩到 [0.0,1.0] 区间。

改变颜色映射关系可以从不同的层面来可视化属性与目标之间的关系



##### 回归问题使用关联热图：

是不同属性之间的相关性和属性与目标之间的相关性，代码如下：

```
        #calculate correlation matrix 
        corMat = DataFrame(abalone.iloc[:,1:9].corr())
        #print correlation matrix 
        print(corMat)
```

如果是实数值预测，那么在计 算关系矩阵时可以包括目标值。

回归问题与分类问题的本质差别