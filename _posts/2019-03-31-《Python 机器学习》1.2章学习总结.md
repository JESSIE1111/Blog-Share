---
layout:     post
title:      "《Python 机器学习》1.2章学习总结"
date:       2019-03-31
author:     "董国珍"
header-img: "img/post-bg-2015.jpg"
tags:
    - 数据分析
---



#  <center>《Python 机器学习》1.2章学习总结<center>  

##  <center>本书简介<center>

* 在学习和研究机器学习的时候，面临令人眼花缭乱的算法，机器学习新手往往会不知所措。本书从算法和Python 语言实现的角度，帮助读者认识机器学习。本书专注于两类核心的“算法族”，即惩罚线性回归和集成方法，并通过代码实例来展示所讨论的算法的使用原则。全书共分为7 章，详细讨论了预测模型的两类核心算法、预测模型的构建、惩罚线性回归和集成方法的具体应用和实现。  
   
   
<img src="/Blog-Share/img/1903/04/Mialia/book.png" width="500" hegiht="900" align=center />    
   
## 前言

惩罚线性回归：对最小二乘法回归的改善和提高（预测分析的首选）  
   
集成方法：克服二元决策树的局限性进化而来（过定问题 <经查询，大致是指过定点的问题> ）

## 第一章 关于预测的两类核心算法
  

### 问题和所用方法完整描述  
  
有监督学习 => 函数逼近问题（即本书涉及的机器学习问题：常用线性回归和逻辑回归）=> 两类算法：*惩罚线性回归、集成方法*   

* 1.1 这两类算法有用：(对各种不同规格的数据集都接近最优) 
  
> 术语：
>
> 正例（输出结果是正向的）
> 非平衡 （某类数据远大于其他类的）
> 属性=特征=变量
>
> (当数据含有大量的特征，但是没有足够多的数据或时间来训练更复杂的集成方法模型时，惩罚回归方法将优于其他算法。)  
>
> (惩罚回归训练快，预测快，许多情况下可以提供最佳答案)  
>
> (算法可以在特征提取（选择哪些变量用于预测结果的过程）中，根据对预测结果的贡献程度对特征打分，抛掉主观臆断，增加预测过程的确定性)  

* 1.2 惩罚回归

> 最小二乘法根本缺陷：会过拟合（自由度=点数时：预测效果垃圾）
>
> 惩罚回归可以减少自由度，使其与数据规模，问题复杂度匹配

* 1.3 集成方法

> 基本思想：构建多个不同的预测模型，将它们做某种组合作为最终输出  
>
> 基学习器：单个预测模型  （常为二元决策树）  
>
> 投票bagging （自举集成算法简化说法）：对训练数据随机取样，根据子集进行训练->可产生大量稍有差距的二元决策树  

* 1.4 算法的选择
! [text ] (/Blog-Share/img/1903/04/Mialia/compare.jpg)
> ![avatar](/Blog-Share/img/1903/04/Mialia/compare.jpg)  
>
> (GB级训练集：惩罚线性回归0.5h，集成5h)  
>
> 迭代、试错法  

* 1.5 构建预测模型的流程

> 浏览数据（统计意义上的检测分析）-> 选择特征 -> 开始训练，产生模型 -> 估计性能 -> 调整特征集（调整目标函数）->确定模型  
> ![avatar](/Blog-Share/img/1903/04/Mialia/1.5.png)  
> （问题可能会需要重构） 
>
> * 特征提取和特征工程
> > 特征提取：把自由形式的数据转换成行、列形式的数字的过程  
> > 特征工程：对特征进行整理组合
> >
> > (数据准备和特征工程估计会占开发一个机器学习模型 80% ～ 90% 的时间,通常训练100-5000个不同的模型)

* 1.6 各章内容及其依赖关系

> ![avatar](/Blog-Share/img/1903/04/Mialia/1.6.png)  
>
> 样本外数据（测试数据）  


## 第二章 通过理解数据来了解问题

### 部分数据分析的方法和工具

* 2.1 新问题

> * 属性（预测因子/特征/独立变量/输入）->分类：数值、类别
> * 标签（结果/目标/依赖变量/响应）
> 数据集需要检查：
> > 1.规模（行列数）:判断训练时间、列数远大于行数<惩罚线性回归可能最佳>  
> > 2.类额变量的数目、取值范围:  
> > 3.缺失的值:删除或填补（可用预测）  
> > 4.属性和标签的统计特征:异常值（四分位数等/分位数图<probplot函数>），异常点（可加重比例，单独训练或者删除）  

* 2.2 分类问题（声纳发现鱼雷）

> （2.2.4前笔记参上）  
>
> 类别属性的统计特征：可分层抽样（P32）

* 2.3 可视化展示

> 平行坐标图（parallel coordinates plot）：多条折线（不同属性对应）：观测属性和标签之间的的关系  
>
> 交会图（cross-plots）:即散点图（scatter plots）   
> <tips:对重叠的点可以给标签值加上小的随机数<扰动>/调节画图的透明度<半透明>（alpha=0.5）>  
>
> 热图（heat map）：<对大量属性>将Pearson相关系数构成一个矩阵  

* 2.4 基于因素变量的实数值预测（鲍鱼的年龄）

> 画盒型图时可先归一化（实现取值范围的缩放）
>
> 分对数转换

* 2.5 用实数值属性预测实数值目标：红酒口感评估

> (注意边缘点)

* 2.6 多类别分类问题：玻璃种类
>
> 多类别分类类似二元分类问题，区别在于有几个（而非两个）离散的（红酒的有序，这个没有）输出
>
