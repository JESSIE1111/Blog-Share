---
layout:     post
title:      "集成方法与惩罚线性回归简介"
date:       2019-04-01
author:     "闫怡君"
tags:
    - python
    - 学习笔记
---
## 集成方法

- 基本思想：构建多个不同的预测模型，然后将其输出做某种组合作为最终的输出（如取平均值或采用多人的意见）。

- 单个预测模型：基学习器

- 集成方法的提出：某些机器学习算法输出结果不稳定。

- 基学习器：二元决策树

- 理论基础：来自于Kearns和Valiant提出的基于PAC（probably approximately correct）的可学习性理论。

- PAC 定义了学习算法的强弱：

   弱学习算法：识别错误率小于1/2(即准确率仅比随机猜测略高的算法)；

   强学习算法：识别准确率很高并能在多项式时间内完成的算法

   强可学习与弱可学习是等价的，即：一个概念是强可学习的充要条件是这个概念是弱可学习的。 据此，为了得到一个优秀的强学习模型，我们可以将多个简单的弱学习模型“提升”。、

- 两个主要问题：  

  如何改变数据的分布或权重  

  如何将多个弱分类器组合成一个强分类器

- 目前主流方法有三种：  

  1、 Boosting方法：包括Adaboosting，提升树（代表是GBDT）, XGBoost等  

  2、 Bagging方法：典型的是随机森林  

  3、 Stacking算法


### 集成方法-Bagging的思想介绍

- 对于给定的训练样本S,每轮从训练样本S中采用有放回抽样(Booststraping)的方式抽取M个训练样本,共进行n轮，得到了n个样本集合，需要注意的是这里的n个训练集之间是相互独立的。

- 在获取了样本集合之后，每次使用一个样本集合得到一个预测模型，对于n个样本集合来说，我们总共可以得到n个预测模型。

- 如果我们需要解决的是分类问题，那么我们可以对前面得到的n个模型采用投票的方式得到分类的结果，对于回归问题来说，我们可以采用计算模型均值的方法来作为最终预测的结果。




### 什么时候采用bagging集成方法？

学习算法不稳定时：

+  if small changes to the training set cause large changes in the learned classifier.

   （也就是说如果训练集稍微有所改变就会导致分类器性能比较大大变化那么我们可以采用bagging这种集成方法）

+  If the learning algorithm is unstable, then Bagging almost always improves performance.

   (当学习算法不稳定的时候，Bagging这种方法通常可以改善模型的性能)




### 集成方法-Boosting的思想介绍

- Boosting方法是一种典型的基于bootstrapping思想的应用

- 特点：每一次迭代时训练集的选择与前面各轮的学习结果有关，而且每次是通过更新各个样本权重的方式来改变数据分布。

- 总结：

  分步学习每个弱分类器，最终的强分类器由分步产生的分类器组合而成  

  根据每步学习到的分类器去改变各个样本的权重（被错分的样本权重加大，反之减小)



### 集成方法-stacking的思想介绍

- stacking是一种组合多个模型的方法，它主要注重分类器的组合。stacking采用的是基本模型非线性组合的方式。

- 具体过程如下：

  + 划分训练数据集为两个不相交的集合。 
  + 在第一个集合上训练多个学习器。 
  + 在第二个集合上测试这几个学习器 
  + 将第三步得到的预测结果作为输入，将正确的映射作为输出，训练一个更高层的分类器

---



## 惩罚线性回归

- 基本思想：克服最小二乘法的基本缺陷（过拟合）。

- 自由度：统计学上的自由度是指以样本的统计量来估计总体的参数时，样本中独立或能自由变化的自变量的个数。

- 惩罚线性回归：减少自由度使之与数据规模、问题的复杂度相匹配。对于具有大量自由度的问题，惩罚线性回归方法获得了广泛的应用。

- 适用于处理自由度大的问题：基因问题；文本分类问题。




### 构建惩罚线性回归模型的一般步骤：

- 第一步：在整个数据集上训练获得系数曲线。

- 第二步：运行交叉验证来寻找最佳的样本外性能，并提供该性能对应的模型。

------



## 算法选择

>- [ ] 惩罚线性回归的优势：训练速度非常快，预测速度快，适用于处理大量特征
>
>- [ ] 劣势：性能上不如集成方法
>
>- [ ] 集成方法的优势：处理问题复杂度高
>
>- [ ] 劣势：训练速度慢、预测速度慢、不适合处理大量特征
>

------



#### 参考资料：

1、CSDN（作者：nini_coded）:https://blog.csdn.net/nini_coded/article/details/79341485 

2、简书（作者：Amica）：https://www.jianshu.com/p/c4bf8821af19

3、pyhon机器学习
