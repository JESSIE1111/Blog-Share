---
layout:     post
title:      "《Python 机器学习》第7章学习总结"
date:       2019-05-06
author:     "董国珍"
header-img: "img/post-bg-2015.jpg"
tags:
    - 数据分析
---



# 《Python 机器学习》第7章学习总结 
   

## 第7章 用 Python构建集成模型

使用集成方法解决与第5章同样的问题（第2章提出）+各种算法的对比总结

* 7.1 用 Python 集成方法工具包解决回归问题
 
Python scikit-learn 算法包集成方法模块中有随机森林和梯度提升法，这两个算法都可以用来解决回归问题  
（梯度提升使用误差最小化的方法来构建决策树）   
（Bagging 和随机森林是使用减少方差的方法）  
  
> 
> 1. 随机森林模型
> >
> > * 常用参数(参考Python scikit-learn 算法包的官方文档，及搜索各类博客教程)
> >
> > n_estimators : 整型,可选（缺省值为10）,指定集成方法中决策树的数目,比较好的尝试是100 ～ 500  
> >
> > max_depth : 整型或者 none，可选（缺省值为 None)，指定决策树的深度（不设置时，性能提升，但牺牲了训练时间）  
> >
> > max_leaf_nodes : 指定决策树的叶子节点数（如果指定了max_leaf_nodes，max_depth 参数就会被忽略 ）  
> >
> > min_sample_split : 整型，可选（缺省值为2），当节点含有的数据实例少于 min_sample_split 时，此节点不再分割  
> >
> > min_samples_leaf : 整型，可选（缺省值为 1）,与min_sample_split含义类似  
> >
> > max_features : 整型、浮点型或字符串型，可选（缺省值为 None),查找最佳分割点时，需要考虑多少个属性是由 max_features 参数和问题中一共有多少个属性共同决定的(假设问题数据集中共有 nFeatures 个属性)  
> >
> > > 
> > > * 如果max_features 是整型，则在每次分割时考虑max_features个属性。注： 如果 max_features > nFeatures，则抛出错误  
> > > 
> > > * 如果max_features 是浮点型，max_features 表示需考虑的属性占全部属性的百分比，即 int( max_features * nFeatures)  
> > > 可选择的字符串值如下：    
> > > auto max_features=nFeatures     sqrt max_features=sqrt(nFeatures)<推荐回归模型中使用>     log2 max_features=log2(nFeatures)   
> > >
> > > * 如果 max_features=None, 则 max_features=nFeatures  
> > > 
> > random_state : 整型，RandomState实例，或者 None ( 缺省值为 None),随机数种子。（在真实的模型训练阶段，可将random_state 设为缺省值None。固定 random_state 值就固定了测试集，这样重复的参数调整和训练最终是对测试数据集的过度训练）  
> >
> > feature_importances : 数组，长度等于问题的属性数,数组中的值是正的浮点数，表明对应的属性对预测结果的贡献重要性(属性的重要性由 Brieman 在 最初的随机森林论文中所提的一个方法来确定。基本思想是，每次选中一个属性，然 后对属性的值进行随机置换，记录下预测准确性的变化，预测的准确性越高，此属性也越重要。 ),None表示各属性权重相同  
> >
> 常用sklearn.metrics 的 mean_squared_error 函数来计算预测均方误差，并将其保存在一个列表中  
> 
> 随机森林可以估计每个属性对预测准确率的贡献（重要性）  
>
> 2. 梯度提升法  
>
> >
> > * 常用参数  
> >
> > Loss  : 字符串，可选（缺省值为“ls”)，包含Learning_rate  
> >
> > N_estimators : 指定集成方法中的决策树数目  
> >
> > Subsample : 浮点型，可选（缺省值为1.0）(如果与随机森林相似，用数据的抽样对决策树进行训练，则梯度提升法变成了随机梯度提升法),推荐取值0.5    
> >
> > Max_depth : 整型，可选（缺省值为 3）  
> >
> > Max_features : 整型、浮点型、字符串，或者 None, 可选（缺省值为 None）  
> >
> > Warm_start : 布尔型，可选（缺省值为 False）。 如果 warm_start 设为 True，fit() 函数将从上次训练停止的地方开始   
> >
> > * 常用属性  
> >
> >  Feature_importances : 数组中的值是正的浮点数，表明了相应属性对预测结果的重要性，数值越大，表明此属性越重要    
> >
> > Train_score : 数组，其长度等于集成方法中决策树的数目,此数组存放在训练阶段对决策树依次训练时的误差    
> >
> > * 常用方法
> >
> > Fit(XTrain, yTrain, monitor=None)   
> >
> >  Predict(X)   
> >
> > Staged_predict(x) : 与 predict() 函数的行为类似，可迭代  
> >

* 7.2 用 Bagging 来预测红酒口感  

自举取样（bootstrap sample）:基于取样样本训练决策树，将决策树输出（结果）取平均值作为模型最终的输出（结果），叫作 Bagging，是一个纯粹的减少方差的技术  

Bagging 从 输入数据中自举取样,这种取样是放回取样，因此有些数据是重复的  

* 7.3 Python 集成方法引入非数值属性  

有看法认为，随机森林适合于规模很大且很稀疏的属性空间，如文本挖掘问题  

* 7.4 用 Python 集成方法解决二分类问题  

> 1. 随机森林分类器：  
> 
> > * RandomForestClassifier常用参数（与RandomForestRegressor 中不同的）  
> >  
> > Criterion : 字符串，可选（缺省值为“gini”)。 可能的取值: Gini ：利用基尼不纯度（Gini impurity）；Entropy ：利用基于熵的信息增益  
> >
> > * RandomForestClassifier常用方法  
> >
> > Fit( X, y, sample_weight=None) : 对于有 nClass 个不同类别的 多类别分类问题，标签是从 0 到 nClass-1 的整数    
> >
> > Predict(X) 
> >
> > Predict_proba(X) : 这个版本的预测函数产生一个二维数组  
> >
> > Predict_log_proba(X) : 这个版本的预测函数产生一个与 predict_proba 相似的二维数组  
> 
> 对于二分类问题，评价标准可以选择 ROC 曲线下面积（AUC），或者误分类率(推荐使用 AUC，因为它能给出整体的性能评价）  
> AUC 值越大越好，所以不是寻找曲线中的波谷， 而是找波峰  
> 
> 2. 梯度提升法  
> 
> > * GradientBoostingClassifer常用参数（与 GradientBoostingRegressor 中不同的）  
> >  
> > loss : deviance　对于分类问题，deviance 是缺省的，也是唯一的选项  
> > 
> > * GradientBoostingClassifer常用方法  
> > 
> > fit（X,y , monitor=None)   
> > 
> > decision_function(X) : 梯度提升分类器会产生与所属类别的概率相关的实数估计值,这些估计值还需要经过反 logistic 函数将其转换为概率。转换前的实数估计值可通过此函数获得  
> > 
> > predict_proba(X) : 此函数预测所属类别的概率。它对于每个类别有一列概率值  
> > 
> > 上述函数的阶段性（staged）版本是可迭代的，产生与决策树数目相同的结果（也与 训练过程中执行的步数一致）   
> > 

* 7.5 用 Python 集成方法解决多类别分类问题   

> 处理类不均衡问题   
> 
> 如果随机取样没有充分考虑样本的代表性，会导致抽样后的各类别样本所占比例与原始数据中的比例有很大的区别。为了避免这个问题，代码中采用分层抽样技术   
> 
