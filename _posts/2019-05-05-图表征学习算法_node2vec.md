---
layout:     post
title:      "图表征学习算法_node2vec"
date:       2019-05-05
author:     "林柯秀"
header-img: "img/post-bg-2015.jpg"
tags:
    - 图表征学习
    - 特征工程
---

图（graph）普遍用于对网络场景进行数学建模。常见的节点分类、链接预测以及社区检测等任务均为图建模期望解决的经典问题。

当面对大规模网络时对其进行分析是富有挑战性的。常见的机器学习方法很难在图上拥有良好的表现。因此，学习网络中节点的特征成为了需要重点解决的问题之一。

Aditya Grover 和 Jure Leskovec 在2016年提出的 node2vec 算法在网络节点特征学习方面表现不俗，本文旨在基于论文 *node2vec：Scalable Feature Learning for Networks* 对 node2vec 算法进行简单介绍。本文首先就skip-gram模型进行介绍，接着讨论node2vec算法的采样策略，最后给出算法的伪代码。

### Skip-Gram 模型

从算法的命名上容易联想到 NLP 领域的一个常见算法 word2vec，node2vec也确实借鉴了word2vec的思想。在 embedding 的处理过程中，其学习方法与word2vec一致，均采用 Skip-Gram 模型。

Skip-Gram模型实际上是n-gram的一个变体，其从中心词上下文的一个窗口中随机选择一定数量的词。其为CBOW模型（Continuous Bag of Words Model）的一个逆过程。

#### Skip-Gram模型的符号定义

- $w_i:$ 词汇 $i$；
- $V\in \mathbb{R}^{n\times |V|}:$ 输入的词矩阵；
- $v_i:$ $V$ 的第 $i$ 列，$w_i$ 的输入向量表示；
- $U\in \mathbb{R}^{n\times |V|}:$ 输出词矩阵；
- $u_i:$ $U$ 的第 $i$ 行，$w_i$ 的输出向量表示；
- $m:$ 窗口的大小 

#### Skip-Gram模型工作流程

1. 生成中心词的独热向量 $x\in \mathbb{R}^{|V|}$;
2. 得到中心词的嵌入词向量 $v_c=Vx\in \mathbb{R}^n$;
3. 生成得分向量 $z=Uv_c$;
4. 将得分向量转换为概率，$\hat{y}=softmax(z)$;
5. 我们希望生成的概率向量贴近 $y^{(c-m)}, \cdots, y^{(c-1)}, y^{(c+1)}, \cdots, y^{(c+m)}$ 的真实概率。

#### Skip-Gram的目标函数

![1557075115257](/Blog-Share/img/1905/01/shuangmulin/1557075115257.png)

对于Skip-Gram模型的更加详细直观的阐述可以参考博文[Word2Vec Tutorial-The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)。

### 采样策略

不同于 word2vec 可以自然地生成嵌入语料库，大多数网络结构是复杂的，其有许多不同于文本句的属性。如果将一句话作为图表示，其结构可以非常简单。

![1557075687286](/Blog-Share/img/1905/01/shuangmulin/1557075687286.png)

为了解决这一问题，node2vec使用了一个可调整的采样策略来抽样有向无环子图。而这个任务的实现有赖于从图的每个节点生成随机游走来完成。

#### 采样策略的参数

- 次数：从图中的每个节点生成的随机游走的次数；
- 步长：每一次随机游走的节点个数；
- P：Return parameter, 决定再访问节点的可能性；
- Q：In-out parameter，调整Q的大小可以用于靠近不同的搜索策略（BFS/DFS），其控制探索图中未发现部分的概率。

![1557076378997](/Blog-Share/img/1905/01/shuangmulin/1557076378997.png)

为了更直观的解释两个超参数的含义，考虑上图所示的情况。在随机游走时，从节点 t 转换到节点 v，考虑网络结构并搜索不同的邻居空间，定义从节点 v 转移到任意邻居节点的概率为<边的权重>$\times$ <$\alpha$>，<$\alpha$>的大小取决于节点 t 到节点 x 的距离。
$$
\alpha_{pq}(t,x) = \left\{
    \begin{aligned}
    \frac{1}{p}   && if \ d_{tx}=0 \\
    1 && if\ d_{tx} = 1 \\
    \frac{1}{q} && if\ d_{tx} =2
    \end{aligned}
\right.
$$
这允许我们调整参数来强调图的局部/全局结构。

由上文我们可以知道转移概率最终取决于三个方面：

- 游走的前一个节点；
- 超参数P和Q；
- 边的权重

这一部分是node2vec的本质。

### node2vec算法

综合以上两个部分，我们得到了node2vec模型，其伪代码如下所示：

![1557077171375](/Blog-Share/img/1905/01/shuangmulin/1557077171375.png)

论文的实验部分做的非常详尽。有节点分类和边预测两个任务，各在三个数据集上进行，同时和谱聚类，DeepWalk，LINE三种算法做了严格的对比实验。同时进行了敏感度分析，抗干扰分析。限于全文的篇幅，本文不对node2vec算法的实验部分进行详细阐述，读者可通过阅读原论文进行了解。

### 参考文献

1. [Grover A, Leskovec J. node2vec: Scalable Feature Learning for Networks[C]// Acm Sigkdd International Conference on Knowledge Discovery & Data Mining. 2016](https://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf)
