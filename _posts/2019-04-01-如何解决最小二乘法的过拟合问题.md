---
layout:     post
title:      "如何解决最小二乘法的过拟合问题"
date:       2019-04-01
author:     "闫怡君"
tags:
    - python
    - 学习笔记
---

### 什么是过拟合？

过拟合是指模型为了得到一致假设而使假设变得过于严格，也就是说模型对训练数据的学习有点过头。模型并没有学习数据的整体分布，而是学习了每个数据点的预期输出。这就好比你在做数学题的时候，你只记准了某些特定问题的答案是什么，但不知道解题的公式。这就造成模型无法泛化。

### 过拟合的常见原因

- 建模样本选取有误，如样本数量太少，选样方法错误，样本标签错误等，导致选取的样本数据不足以代表预定的分类规则；
- 样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则；
- 假设的模型无法合理存在，或者说是假设成立的条件实际并不成立；
- 参数太多，模型复杂度过高；
- 对于决策树模型，如果我们对于其生长没有合理的限制，其自由生长有可能使节点只包含单纯的事件数据(event)或非事件数据(no event)，使其虽然可以完美匹配（拟合）训练数据，但是无法适应其他数据集。
- 对于神经网络模型：a)对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面；b)权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。

### 前向逐步回归

#### 最佳子集选择算法的过程

基本思想：在列的个数上增加一个约束（假设为nCol），然后从X的所有列中抽取特定的个数的列构成数据集，在上边执行最小二乘法，遍历所有列的组合（列数为nCol），找到在测试集上取得最佳效果的nCol值；增加nCol值，重复上述过程。以上过程产生最佳的一列子集、两列子集一直到所有列子集（对应矩阵X）。对于每个子集同样有一个性能与之对比。下一步决定在部署时是使用一列子集版本、两列子集版本，还是其他版本。

缺点：计算量大

模型越复杂，泛化能力越差。

### 通过惩罚回归系数控制过拟合——岭回归

步骤：

- 首先要对数据进行一些预处理，尽量把保持所有特征在一个范围内，使用特征缩放和均值归一化来处理特征值是很有必要的，否则，不同特征的特征值大小是没有比较性的。
- 其次构建惩罚函数，针对不同的λ，画出岭迹图。
- 根据岭迹图，选择要剔除那些特征。


### 小知识点

#### 算法的选择（倾向而非必须）

>- 线性模型：列比行多 或 简单问题  时间短

>- 非线性模型：行比列多 或 复杂问题 时间长

## 参考资料

1. 过拟合[https://baike.baidu.com/item/过拟合/3359778?fr=aladdin#2]
2. python机器学习

